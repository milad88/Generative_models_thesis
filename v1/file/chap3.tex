%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapter3


\pagestyle{fancy} 
\chapter{Discussion}
\label{cha:3}
\vspace{1cm}

\section{Employment of VAEs in generative models for robotics}
\label{sec:VAE_generative}
Lets go now through some papers to see where and how the VAEs have been employed and show their effectiveness in various applications of generative models in robotics. The first one~\cite{eslami2016attend} where a framework called by the authors \textsl{Attend, infer, repeat (AIR)} the VAE structure here is quite different that the encoder was implemented as a Recurrent Neural Network (RNN), since its purpose is to learn to detect and generate objects, specifically “where is the objects, what are they and how many are they”. The additional recurrence to the structure is basically to detect how many objects are present in the input data. Experiments were designed initially on 2D data particularly on multiple MNIST digits, and reliably the model were able to detect and generate the constituent digits from scratch, it shows advantages over state-of-art generative models computationally and also in terms of generalization to unseen datasets. Other Experiments on 3D datasets, considering scenes consisting of only one of three objects: a red cube, a blue sphere, and a textured cylinder. The network accurately and reliably infers the identity and pose of the object, on the other hand, an identical network trained to predict the ground-truth identity and pose values of the training data has much more difficulty in accurately determining the cube’s orientation.\\

Moreover, in robotics, grasping and manipulation of various objects is a critical and challenging problem,~\cite{veres2017modeling} has proposed another concept referred to as the grasp motor image (GMI) that combines object perception and a learned prior over grasp configurations, to synthesize new grasps to apply to a different object. At the core of GMI is the autoencoder structure, taking advantage of Deep  Learning (DL), particularly building both encoder and decoder as Convolutional Neural Networks (CNN). The approach followed is intuitive: based on
perceptual information about an object, and an idea of how an object was previously grasped, collecting a object/grasp pair dataset of successful, cylindrical precision robotic grasps using the V-REP  simulator~\cite{rohmer2013v}, and object files provided by~\cite{kleinhans2015g3db} on a simulated “picking” task. The VAE was trained on this dataset to generate grasps for novel objects. GMI integrates
perceptual information and grasp configurations using deep generative models. Applying it to a simulated grasping task, has demonstrated the capacity of these models to transfer learned knowledge to novel objects under varying amounts of available training data.\\


\section{Exploitation of VAE and GMM in RL}
As mentioned in the previous section, most of the time applying RL algorithm directly on high-dimensional data does not lead to a good performance, so in the kind of situation it is advantageous to make use of the techniques that permit to reduce the dimensionality by representing the data in more suitable way.\\

One of the frameworks I went through has exploit both VAE and GMM to neatly which makes it feasible to fit the dynamics
even when the number of samples is much lower than the
dimensionality of the system. this what~\cite{finn2016deep} does, where initially RL algorithm run on robot with initial random policy to collect N (5 for that experiment) samples, then use them to fit GMM to learn the environment dynamics  or the policy controller without vision but using only the robot’s configuration as the state. In a second phase a VAE is trained to encode image dataset with unsupervised learning to produce a low-dimensional bottleneck vector that is a natural choice for learned feature representation or feature points for each image that concisely describes the configuration of objects in the scene, the interesting part of this VAE is that it is forced to encode spatial features rather than values. This is obtained  basically by applying spatial soft-max activation function that consists of tow operations on the last convolutional layer of the encoder as follow:
\begin{equation}
s_{cij} = \frac {e^{\frac{a_{cij}}{\alpha}}}
{\sum_{\acute{i}\acute{j}} e^{ \frac {a_{c \acute{i} \acute{j}}}{\alpha}}}
\end{equation}
where the temperature $\alpha$ is a learned parameter. Then, the expected
2D position of each softmax probability distribution $s_c$ is
computed according to:
\begin{equation}
f_c = (\sum_i i ∗ s_{cij} , \sum_j j ∗ s_{cij} )
\end{equation}
which forms the autoencoder's bottleneck and essentially it is the learned spatial feature point representation, that will therefore be capable of directly localizing objects in the image. The third and final phase of this framework is same as the first one, but the difference here is that the controller is trained on the feature points of the encoder using same trajectory-centric reinforcement
learning algorithm.\\
The experiments of this method showed that it could be used to learn a wild range of manipulation skills that require close coordination between perception and
action, and uses a spatial feature representation of the environment, which is learned as a bottleneck layer in
an autoencoder. This allows us to learn a compact state from high-dimensional real-world images. Furthermore, since this
representation corresponds to image-space coordinates of objects in the scene, it is particularly well suited for continuous control. The trajectory-centric RL algorithm we employ can learn a variety of manipulation skills with these spatial
representations using only tens of trials on the real robot.\\

In another work~\cite{nair2018visual} a RL framework was designed to jointly learns representations and policies from raw sensor inputs that achieve arbitrary goals under this representation by practicing to reach self-specified random goals during training. Here shows up the problem of choosing a suitable goal representations, to deal with this, a goal space \c{G} as to be same as the state space \c{S}. As well the problem of high-dimensional observations such as images arises, to handle it the authors once again the rely on VAE to learn a latent embedding for both \c{G} and \c{S}, by executing a random policy to collect state observation and optimize Eq.~\ref{eq:VAE_loss}, an additional online training has been introduced where the VAE is fine-tuned during the policy training each 3000 environment steps on all of the images observed by the policy, because as the policy improve it might visit new state observations where the VAE is not trained on, this additional training helped the performance of the overall algorithm. The final step is to run the RL algorithm which is a value-based one in this work twin delayed deep deterministic
policy gradients (TD3)~\cite{fujimoto2018addressing} is used, the thing that should be pointed out here that the negative Mahalanobis distance in the latent space were used as a reward function, but it turned out that minimizing this squared distance was equivalent to maximize the probability of the latent goal. This framework for learning general-purpose goal-conditioned policies that can achieve goals specified with target observations.\\




\clearpage{\pagestyle{empty}\cleardoublepage}