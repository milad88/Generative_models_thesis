%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapter3


\pagestyle{fancy} 
\chapter{Discussion}
\label{cha:3}
\vspace{1cm}

Going through the different frameworks we have seen in the previous section, we could notice that sometimes the goal of some researches are the same, like to find the probability distribution of data, or to maximize the total returned reward, generate new data instances, but the various authors were making different choices of the available techniques to achieve there goal in solving the task. We could explain these choices by making one step back and have a look on the data. Firstly the question that should be made is \textacutedbl what is the raw data the framework is going to cope with?\textgravedbl. The data we have seen are of different natures like trajectories, images, sensory data, grasp configuration and so on. Posting this question initially leads to figure out what are the space dimensions to deal with, and gives place to additional questions like \textacutedbl should we move to latent space?\textgravedbl in case it is high-dimensional like images. Another main question is \textacutedbl How big is the dataset?\textgravedbl this one is very important impact on the choice to make on the technique to use for the generative modeling, I discovered that when the dataset to base on the not large enough, means that it is more opportune to know probability distribution to generate new data instances from, so the choice of VAEs~\ref{cha:VAE} with reparameterizing trick~\ref{cha:VAE_REparam} in this case should be a good fit. Nevertheless, VAEs struggles to find a complex probability distribution in situations where the dataset is high-dimensional and composed of variables that vary in wild ranges, to overcome this kind of issue the GMM~\ref{cha:GMM} approach may fit better, given the property of GMM to combine $K$ probability distributions components to represent a final probability distribution that fits more efficiently the dataset. Moreover, in human cloning and/or learn from demonstrations tasks, where data are collect by making an expert to behave or execute actions in environment to achieve the goal of the task, this collected dataset is usually small for both learning process of the agent given some demonstrations, or cloning an expert behavior to achieve the required task. In this kind of situations where the amount of data available is not big enough, VAEs still not a good choice unless we follow the idea of moving to the latent space, then VAEs is the best option taking advantage its nice probabilistic formulation they come with as a result of maximizing a lower bound on the log-likelihood. Working in latent space is another approach that can circumvent the high-dimension real-world data. VAE naturally collapses most dimensions in the latent representations, and generally getting very interpretable dimensions out, even though the training dynamics are generally a bit weird. VAEs are used as well to learn the data representation like in~\cite{finn2016deep} which is a reinforcement learning application to robotic manipulation tasks. The authors were interested in knowing the spatial feature representation of the environment, these spatial features are the configurations of objects in the scene. The bottleneck of the autoencoder was modified in such way the spatial representation is learned. VAE has shown its ability as well as a generative model like in~\cite{eslami2016attend} formalizing a framework called \textit{Attend, infer, repeat} for efficient variational inference in latent spaces of variable dimensionality. The key idea of this paper is to treat inference as an iterative process, implemented as a recurrent neural network that attends to one object
at a time, and learns to use an appropriate number of inference steps for each image. VAE as a generative model is able to compete with GAN which is the most interesting idea in machine learning of the last 10 as Yann LeCun, Director of AI Research at Facebook AI claims. GANs is a superior model with respect to VAEs because they are better at generating visual features this superiority conduct to say that the adversarial loss is better than mean-squared loss. The difference that products this superiority is that VAE can spread the probability mass to places it might not make sense, whereas GAN models may never explore. Later, a cooperation between VAE and GAN has been done in~\cite{rahmatizadeh2018vision} mentioned in~\ref{sec:GAN-VAEs} which is one of the most advanced paper I read during this survey, that represent the state of art of robot manipulation tasks. it demonstrated how it is possible to learn complex manipulation tasks from user demonstrations, such as picking up a towel, wiping an object, and depositing the towel to its previous position, entirely from raw images with direct behavior cloning and outputs the joint configuration of the robot.\\
After the spread of GANs, their influence reached as well to reinforcement learning algorithms, specially those who follow the actor-critic approach like TRPO and DDPG. However GANS could be interpreted as an actor-critic approach, where the generator should learn the policy and the discriminator acts the critic that returns a feedback in such a way both generator/actor-discriminator/critic improve by the time goes on. Generally speaking this is what happens in all RL approaches that exploit the GANs architecture, like in GAIL~\cite{DBLP:journals/corr/HoE16} which is designed to imitate an expert behavior. The generator in this case is trying to generate trajectories as much similar to the expert ones, until the discriminator could distinguish between them, in other word a saddle point is found. This idea repeats itself each time a framework would like to achieve a cooperation between RL and GAN, as we have seen as well in SI-GARL~\cite{liu2019self}. GAIL and SI-GARL are the best examples to make that describes the introduction of GANs in RL, it transforms the actor-critic approach of reinforcement learning to generative adversarial network, where the generator is acting as it is an actor the discriminator criticizes be distinguishing between authentic and generated data. The great impact of this introduction was avoiding the design of a reward function which is one of the main issues RL suffer giving space to implement new multi-task frameworks for robotics that are able to guarantee the very important generalization property of machine learning like for example in~\cite{rahmatizadeh2018vision}. The initial goal of inventing the generative model was to generate images, achieving great results, afterwards instead of only learn the data and try to find patterns in it as the other machine learning models, it became a way to make the machines first understand the data and then make the decision regard it. The introducing generative modeling in robotics application has had different employments over time, were firstly was used as a data augmentation instrument when generative modeling was essentially used to generate images, then to a tool that extract more fit representations of the data, latent spaces, or features like spatial features, location, pose, and orientation of the objects in the environment the robot is working in. Given the high performance of the generative models in terms of results, led to get more employments in robotics applications. In the most recent works we have seen generative modeling has almost substitute or at least has become the main framework obviously taking advantage of the existing algorithms, as we have seen in various recent works we have mentioned, where the generator is acting as a policy controller which interacts with the robot, and its parameters are updated according to both generative model loss function and the loss function of the existing algorithm (GAIL uses TRPO RL algorithm). Generative models can substitute RL, as in some papers, by instead of generating new datapoints they generate the action to perform by the robot, such as the joint configurations of the robot, or generating the next state given the current one.\\ In addition, the high dimensionality of the domain space is never been helpful in robotics applications, because in all the robotics tasks we are interested in some specific informations inside the input, so we can notice that working in latent space or in some low-dimensional representation of the input data is almost a must. One of the best ways to obtain these representations is VAE as we have seen in many papers mentioned in the previous section. The most powerful generative models use deep learning where essentially they are neural networks that exchange elaborated data and errors -even if the goal is to try to fool each other like GAN- to achieve the final model that is able to generate new instances similar to original dataset. This is what now days is called Deep Generative models. However, the experiments of papers we have seen unfortunately were made in simulated environments like GAIL, showing very good results even in the more complex ones like Ant and Humanoid. While Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-To-End Learning from Demonstration has experiments on real robot providing videos that show how the robot behavior in performing different tasks. Furthermore,~\cite{tobin2018domain} showed how the generated objects has helped the framework to scale to real-word objects to perform grasps.

\clearpage{\pagestyle{empty}\cleardoublepage}