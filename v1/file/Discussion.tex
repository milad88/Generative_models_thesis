%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapter3


\pagestyle{fancy} 
\chapter{Discussion}
\label{cha:3}
\vspace{1cm}

Going through the different frameworks we have seen in the previous section, we could notice that sometimes the goal of some researches are the same, like to find the probability distribution of data, or to maximize the total returned reward, generate new data instances, but the various authors were making different choices of the available techniques to achieve there goal in solving the task. We could explain these choices by making one step back and have a look on the data. Firstly the question that should be made is \textacutedbl what is the raw data the framework is going to cope with?\textgravedbl. The data we have seen are of different natures like trajectories, images, sensory data, grasp configuration and so on. Posting this question initially leads to figure out what are the space dimensions to deal with, and gives place to additional questions like \textacutedbl should we move to latent space?\textgravedbl in case it is high-dimensional like images. Another main question is \textacutedbl How big is the dataset?\textgravedbl this one is very important impact on the choice to make on the technique to use for the generative modeling, I discovered that when the dataset to base on the not large enough, means that it is more opportune to know probability distribution to generate new data instances from, so the choice of VAEs~\ref{cha:VAE} with reparameterizing trick~\ref{cha:VAE_REparam} in this case should be a good fit. Nevertheless, VAEs struggles to find a complex probability distribution in situations where the dataset is high-dimensional and composed of variables that vary in wild ranges, to overcome this kind of issue the GMM~\ref{cha:GMM} approach may fit better, given the property of GMM to combine $K$ probability distributions components to represent a final probability distribution that fits more efficiently the dataset. Moreover, in human cloning and/or learn from demonstrations tasks, where data are collect by making an expert to behave or execute actions in environment to achieve the goal of the task, this collected dataset is usually small for both learning process of the agent given some demonstrations, or cloning an expert behavior to achieve the required task. In this kind of situations where the amount of data available is not big enough, VAEs still not a good choice unless we follow the idea of moving to the latent space, then VAEs is the best option. Working in latent space is another approach that can circumvent the high-dimension real-world data. VAE naturally collapses most dimensions in the latent representations, and generally getting very interpretable dimensions out, even though the training dynamics are generally a bit weird. VAEs are used as well to learn the data representation like in~\cite{finn2016deep} which is a reinforcement learning application to robotic manipulation tasks. The authors were interested in knowing the spatial feature representation of the environment, these spatial features are the configurations of objects in the scene. The bottleneck of the autoencoder were modified in such way the spatial representation is learned. VAE has shown its ability as well as a generative model it is able to compete w generate new data instances 

\clearpage{\pagestyle{empty}\cleardoublepage}